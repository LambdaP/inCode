<!DOCTYPE HTML>
<html><head><title>Practical Dependent Types in Haskell: Type-Safe Linear Algebra · in Code</title><meta name="description" content="Weblog of Justin Le, covering his various adventures in programming and explorations in the vast worlds of computation physics, and knowledge."><meta http-equiv="Content-Type" content="text/html;charset=utf-8"><meta name="viewport" content="width=device-width,initial-scale=1.0"><meta property="og:site_name" content="in Code"><meta property="og:description" content="Whether you like it or not, programming with dependent types in Haskell moving slowly but steadily to the mainstream of Haskell programming. In the current state of Haskell education, dependent types are often considered topics for advanced Haskell users. However, I can definitely foresee a day where the ease of use of modern Haskell libraries relying on dependent types as well as their ubiquitousness forces programming with dependent types to be an integral part of regular intermediate (or even beginner) Haskell education, as much as Traversable or Maps. So, the point of this post is to show some practical examples of using dependent types in the real world, and to also walk through the why and high-level philosophy of the way you structure your Haskell programs. It’ll also hopefully instill an intuition of a dependently typed work flow of exploring how dependent types can help your current programs. The first project in this series will build up to type-safe artificial neural network implementations. Hooray! There are other great tutorials I’d recommend online if you want to explore dependent types in Haskell further, including this great servant tutorial. Also, I should provide a disclaimer — I’m also currently exploring all of this as I’m going along too. It’s a wild world out there. Join me and let’s be a part of the frontier!"><meta property="og:type" content="article"><meta property="og:title" content="Practical Dependent Types in Haskell: Type-Safe Linear Algebra"><meta property="og:image" content="http://blog.jle.im/img/site_logo.jpg"><meta property="og:locale" content="en_US"><meta property="og:url" content="http://blog.jle.im/entry/practical-dependent-types-in-haskell-linear-algebra.html"><meta name="twitter:card" content="summary"><meta name="twitter:creator:id" content="mstk"><link rel="author" href="https://plus.google.com/107705320197444500140"><link rel="alternate" type="application/rss+xml" title="in Code (RSS Feed)" href="http://feeds.feedburner.com/incodeblog"><link rel="canonical" href="http://blog.jle.im/entry/practical-dependent-types-in-haskell-linear-algebra.html"><link href="http://blog.jle.im/favicon.ico" rel="shortcut icon"><link href="http://blog.jle.im/css/toast.css" rel="stylesheet" type="text/css"><link href="http://blog.jle.im/css/font.css" rel="stylesheet" type="text/css"><link href="http://blog.jle.im/css/main.css" rel="stylesheet" type="text/css"><link href="http://blog.jle.im/css/page/entry.css" rel="stylesheet" type="text/css"><link href="http://blog.jle.im/css/pygments.css" rel="stylesheet" type="text/css"><script type="text/javascript">var page_data = {};
var disqus_shortname='incode';
</script><script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');
ga('create', 'UA-443711-8', 'jle.im');
ga('send', 'pageview');
</script><script type="text/javascript" src="//ajax.googleapis.com/ajax/libs/jquery/1.10.2/jquery.min.js"></script><script type="text/javascript" src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5234d67a6b68dcd4"></script><script type="text/javascript" src="http://blog.jle.im/js/common.js"></script><script type="text/javascript" src="http://blog.jle.im/js/page/entry_toc.js"></script><script type="text/javascript" src="http://blog.jle.im/js/disqus.js"></script><script type="text/javascript" src="http://blog.jle.im/js/disqus_count.js"></script><script type="text/javascript" src="http://blog.jle.im/js/social.js"></script><script type="text/javascript" src="http://blog.jle.im/js/jquery/jquery.toc.js"></script><script type="text/javascript" src="http://blog.jle.im/purescript/entry.js"></script></head><body><div id="fb-root"><script>(function(d, s, id) {
 var js, fjs = d.getElementsByTagName(s)[0];
 if (d.getElementById(id)) return;
 js = d.createElement(s); js.id = id;
 js.src = "//connect.facebook.net/en_US/all.js#xfbml=1&appId=641852699171929";
 fjs.parentNode.insertBefore(js, fjs);
}(document, 'script', 'facebook-jssdk'));
</script></div><div id="header-container"><div id="navbar-container" class="tile"><nav id="navbar-content"><div class="nav-info"><h1 class="site-title"><a href="http://blog.jle.im/" class="nav-title">in Code</a></h1><span class="nav-author">Justin Le</span></div><ul class="nav-links"><li><a href="http://blog.jle.im/">home</a></li><li><a href="http://blog.jle.im/entries.html">archives</a></li><div class="clear"></div></ul></nav></div><div id="header-content"></div></div><div id="body-container" class="container"><div id="main-container" class="grid"><div class="entry-section unit span-grid" role="main"><article class="tile article"><header><div class="unposted-banner">Unposted entry</div><h1 id="title">Practical Dependent Types in Haskell: Type-Safe Linear Algebra</h1><p class="entry-info">by <a class="author" href="http://blog.jle.im/">Justin Le</a></p><p><span class="source-info"><a class="source-link" href="https://github.com/mstksg/inCode/tree/master/copy/entries/dependent-haskell-1.md">Source</a><span class="info-separator"> &diams; </span><a class="source-link" href="http://blog.jle.im/entry/practical-dependent-types-in-haskell-linear-algebra.md">Markdown</a><span class="info-separator"> &diams; </span><a class="source-link" href="http://blog.jle.im/entry/practical-dependent-types-in-haskell-linear-algebra.tex">LaTeX</a><span class="info-separator"> &diams; </span></span>Posted in <a href="http://blog.jle.im/entries/category/@haskell.html" class="tag-a-category" title="Functional, pure, non-strict, statically and strongly typed, natively
compiled…really just the king of great languages.">Haskell</a>, <a href="http://blog.jle.im/entries/category/@ramblings.html" class="tag-a-category" title="My slight ramblings on subjects of interest (to me and hopefully to you
too!). Lots of surveys and introducts to new subjects.">Ramblings</a><span class="info-separator"> &diams; </span><a class="comment-link" href="#disqus_thread">Comments</a></p></header><hr><aside class="contents-container"><h5 id="contents-header">Contents</h5><div id="toc"></div></aside><div class="main-content copy-content"><p>Whether you like it or not, programming with dependent types in Haskell moving slowly but steadily to the mainstream of Haskell programming. In the current state of Haskell education, dependent types are often considered topics for “advanced” Haskell users. However, I can definitely foresee a day where the ease of use of modern Haskell libraries relying on dependent types as well as their ubiquitousness forces programming with dependent types to be an integral part of regular intermediate (or even beginner) Haskell education, as much as Traversable or Maps.</p>
<p>So, the point of this post is to show some practical examples of using dependent types in the real world, and to also walk through the “why” and high-level philosophy of the way you structure your Haskell programs. It’ll also hopefully instill an intuition of a dependently typed work flow of “exploring” how dependent types can help your current programs.</p>
<p>The first project in this series will build up to type-safe artificial neural network implementations. Hooray!</p>
<p>There are other great tutorials I’d recommend online if you want to explore dependent types in Haskell further, including <a href="http://www.well-typed.com/blog/2015/11/implementing-a-minimal-version-of-haskell-servant/">this great servant “tutorial”</a>. Also, I should provide a disclaimer — I’m also currently exploring all of this as I’m going along too. It’s a wild world out there. Join me and let’s be a part of the frontier!</p>
<h2 id="toy-example-linear-algebra">Toy Example: Linear Algebra</h2>
<p>We’re going to build up to type-safe neural networks in Haskell, but to start off, let’s go over a really really small example of dependent types in Haskell by applying it to the “hello world” of dependent types: linear algebra.</p>
<p>We’re going to do two simple linear algebra concepts, and see why it’s super scary to program without dependent types (and wonder how we ever survived without them), and then add in a bit of dependent types to the rescue!</p>
<p>For the most part, we’ll be using the awesome <em><a href="http://hackage.haskell.org/package/hmatrix">hmatrix</a></em> library, which offers both “type-safe” and “non-type-safe” API’s, so we can better compare the two approaches.</p>
<h2 id="the-scary-world-of-unsafe-code">The Scary World of Unsafe Code</h2>
<p>The first thing we’re going to do is simple:</p>
<ol type="1">
<li>Read in two vectors from stdin</li>
<li>Print the dot product of the two</li>
</ol>
<div class="sourceCode"><pre class="sourceCode haskell"><code class="sourceCode haskell">readList<span class="ot"> ::</span> <span class="dt">IO</span> [<span class="dt">Double</span>]
readList <span class="fu">=</span> concat <span class="fu">.</span> readMaybe <span class="fu">&lt;$&gt;</span> getLine

<span class="ot">dotStdin ::</span> <span class="dt">IO</span> ()
dotStdin <span class="fu">=</span> <span class="kw">do</span>
    v1 <span class="ot">&lt;-</span> vector <span class="fu">&lt;$&gt;</span> readList
    v2 <span class="ot">&lt;-</span> vector <span class="fu">&lt;$&gt;</span> readList
    print <span class="fu">$</span> l1 <span class="fu">&lt;.&gt;</span> l2</code></pre></div>
<p>Now, already you should be feeling sweaty. The dot product, <code>&lt;.&gt;</code>, is actually <em>undefined</em> for two vectors of different lengths. Our code here is…unsafe! Runtime errors can happen that are unchecked by the compiler. To fix this in an unsafe language, we’d have to:</p>
<ol type="1">
<li>Be able to recognize by our own human reasoning that there is a potential for a runtime error.</li>
<li>Restructure control flow to avoid situations where the runtime error would occur.</li>
<li>Somehow prove to ourselves, as humans, that our new solution is safe. Then trust our human verification of our human proof.</li>
</ol>
<p>Now, (2) might be cumbersome in some cases, but at least it’s usually a mechanical process. It’s (1) and (3) that are killer. There is no way I trust myself to recognize <em>every</em> potential runtime error. And, in the case that I <em>do</em> actually catch opportunities for runtime errors, I definitely don’t always trust myself that my fix works. If only the compiler could handle (1) and (3) for us!</p>
<p>The second task we’ll do is build up a chain of matrices, and then ask for a vector from stdin to multiply by each one successively.</p>
<div class="sourceCode"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span class="ot">readChain ::</span> <span class="dt">IO</span> [<span class="dt">Matrix</span> <span class="dt">Double</span>]
readChain <span class="fu">=</span> <span class="kw">do</span>
    l <span class="ot">&lt;-</span> getLine
    <span class="kw">case</span> fromLists <span class="fu">&lt;$&gt;</span> readMaybe l <span class="kw">of</span>
      <span class="dt">Just</span> m  <span class="ot">-&gt;</span> (m<span class="fu">:</span>) <span class="fu">&lt;$&gt;</span> readChain
      <span class="dt">Nothing</span> <span class="ot">-&gt;</span> return []
    
<span class="ot">collapseChain ::</span> <span class="dt">Vector</span> <span class="dt">Double</span> <span class="ot">-&gt;</span> <span class="dt">Matrix</span> <span class="dt">Double</span> <span class="ot">-&gt;</span> <span class="dt">Vector</span> <span class="dt">Double</span>
collapseChain v []     <span class="fu">=</span> v
collapseChain v (m<span class="fu">:</span>ms) <span class="fu">=</span> collapseChain (m <span class="fu">#&gt;</span> v) ms

<span class="ot">chainMats ::</span> <span class="dt">IO</span> ()
chainMats <span class="fu">=</span> <span class="kw">do</span>
    ms <span class="ot">&lt;-</span> readChain
    v  <span class="ot">&lt;-</span> vector <span class="fu">&lt;$&gt;</span> readList
    print <span class="fu">$</span> collapseChain v ms</code></pre></div>
<p>Now, this should be giving you heart attacks. Matrix-vector multiplication is only defined when the number of columns the matrix has is the number of rows the vector has. If <em>any</em> of the matrices along the chain is the wrong dimension, the entire thing will blow up.</p>
<h2 id="neural-networks">Neural Networks</h2>
<p><a href="https://en.wikipedia.org/wiki/Artificial_neural_network">Artificial neural networks</a> have been somewhat of a hot topic in computing recently. At their core they involve matrix multiplication and manipulation, so they do seem like a good candidate for a dependent types. Most importantly, implementations of training algorithms (like back-propagation) are tricky to implement correctly — despite being simple, there are many locations where accidental bugs might pop up when multiplying the wrong matrices, for example.</p>
<p>However, it’s not always easy to gauge before-the-fact what would or would not be a good candidate for adding dependent types to, and often times, it can be considered premature to start off with “as powerful types as you can”. So we’ll walk through a simple implementation <em>without</em>, and see all of the red flags that hint that you might want to start considering stronger types.</p>
<h2 id="vanilla-types">Vanilla Types</h2>
<figure>
<img src="/img/entries/dependent-haskell-1/ffneural.png" title="Feed-forward ANN architecture" alt="Feed-forward ANN architecture" /><figcaption>Feed-forward ANN architecture</figcaption>
</figure>
<p>We’re going to be implementing a feed-forward neural network, with back-propagation training. A feed-forward neural network consists structurally of layers of “nodes”, each connected to the each of the nodes of the previous layer and each of the nodes of the next layer. The most important feature of the network itself is the “strength” of these connections, called “weights”. To make a prediction, each node takes, as an input, the weighted sum of all of the outputs of the previous layer, weighted by the connection weights (plus an additional “bias” shift). It then outputs a function of this weighted sum, <img style="vertical-align:middle" src="http://chart.apis.google.com/chart?cht=tx&amp;chf=bg,s,FFFFFF00&amp;chl=f%28x%29" alt="f(x)" title="f(x)" />, to be used by all of the nodes of the next layer. At the high-level, the user feeds in an input vector to the top-level nodes, the network processes these layer-by-layer, and the result of the final nodes is what is taken as the network’s output. The “goal” of designing/training a network is to somehow pick the right set of weights that will give the output that you want for the given input.</p>
<p>While it’s nice to think about neural networks in terms of their nodes, it makes more sense computationally to only identify a network by simply the matrices of weights alone — let’s imagine one “layer”, which is actually a matrix of weights from one layer to another:</p>
<div class="sourceCode"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span class="co">-- source: https://github.com/mstksg/inCode/tree/master/code-samples/dependent-haskell/NetworkUntyped.hs#L10-13</span>
<span class="kw">data</span> <span class="dt">Weights</span> <span class="fu">=</span> <span class="dt">W</span> {<span class="ot"> wBiases  ::</span> <span class="fu">!</span>(<span class="dt">Vector</span> <span class="dt">Double</span>)
                 ,<span class="ot"> wWeights ::</span> <span class="fu">!</span>(<span class="dt">Matrix</span> <span class="dt">Double</span>)
                 }
  <span class="kw">deriving</span> (<span class="dt">Show</span>, <span class="dt">Eq</span>)</code></pre></div>
<p>Now, a <code>Weights</code> linking a layer of <img style="vertical-align:middle" src="http://chart.apis.google.com/chart?cht=tx&amp;chf=bg,s,FFFFFF00&amp;chl=n" alt="n" title="n" /> nodes to a layer of <img style="vertical-align:middle" src="http://chart.apis.google.com/chart?cht=tx&amp;chf=bg,s,FFFFFF00&amp;chl=m" alt="m" title="m" /> nodes will have a bias vector of size <img style="vertical-align:middle" src="http://chart.apis.google.com/chart?cht=tx&amp;chf=bg,s,FFFFFF00&amp;chl=m" alt="m" title="m" /> (the bias shift for each of the output nodes) and a weight matrix of size <img style="vertical-align:middle" src="http://chart.apis.google.com/chart?cht=tx&amp;chf=bg,s,FFFFFF00&amp;chl=m%20%5Ctimes%20n" alt="m \times n" title="m \times n" />.</p>
<p>(We’re using the <code>Matrix</code> type from the awesome <em><a href="http://hackage.haskell.org/package/hmatrix">hmatrix</a></em> library for linear algebra, implemented using blas/lapack under the hood)</p>
<p>Now let’s represent a feed-forward network:</p>
<div class="sourceCode"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span class="co">-- source: https://github.com/mstksg/inCode/tree/master/code-samples/dependent-haskell/NetworkUntyped.hs#L15-18</span>
<span class="kw">data</span> <span class="dt">Network</span> <span class="fu">=</span> <span class="dt">O</span> <span class="fu">!</span><span class="dt">Weights</span>
             <span class="fu">|</span> <span class="fu">!</span><span class="dt">Weights</span> <span class="fu">:&amp;~</span> <span class="fu">!</span><span class="dt">Network</span>
  <span class="kw">deriving</span> (<span class="dt">Show</span>, <span class="dt">Eq</span>)
<span class="kw">infixr</span> <span class="dv">5</span> <span class="fu">:&amp;~</span></code></pre></div>
<p>So a network with one input layer, two inner layers, and one output layer would look like:</p>
<div class="sourceCode"><pre class="sourceCode haskell"><code class="sourceCode haskell">i2h <span class="fu">:&amp;~</span> h2h <span class="fu">:&amp;~</span> <span class="dt">O</span> h2o</code></pre></div>
<p>Where the first component is the weights from the input to the first hidden layer, the second is the weights from the first hidden layer to the second, and the final is the weights from the second hidden layer to the outputs.</p>
<p>TODO: graphs using diagrams?</p>
<p>We can write simple procedures, like generating random networks:</p>
<div class="sourceCode"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span class="co">-- source: https://github.com/mstksg/inCode/tree/master/code-samples/dependent-haskell/NetworkUntyped.hs#L36-46</span>
<span class="ot">randomWeights ::</span> <span class="dt">MonadRandom</span> m <span class="ot">=&gt;</span> <span class="dt">Int</span> <span class="ot">-&gt;</span> <span class="dt">Int</span> <span class="ot">-&gt;</span> m <span class="dt">Weights</span>
randomWeights i o <span class="fu">=</span> <span class="kw">do</span>
  s1 <span class="ot">&lt;-</span> getRandom
  s2 <span class="ot">&lt;-</span> getRandom
  <span class="kw">let</span> wBiases  <span class="fu">=</span> randomVector s1 <span class="dt">Uniform</span> o <span class="fu">*</span> <span class="dv">2</span> <span class="fu">-</span> <span class="dv">1</span>
      wWeights <span class="fu">=</span> uniformSample s2 o (replicate i (<span class="fu">-</span><span class="dv">1</span>, <span class="dv">1</span>))
  return <span class="dt">W</span>{<span class="fu">..</span>}

<span class="ot">randomNet ::</span> <span class="dt">MonadRandom</span> m <span class="ot">=&gt;</span> <span class="dt">Int</span> <span class="ot">-&gt;</span> [<span class="dt">Int</span>] <span class="ot">-&gt;</span> <span class="dt">Int</span> <span class="ot">-&gt;</span> m <span class="dt">Network</span>
randomNet i [] o     <span class="fu">=</span>     <span class="dt">O</span> <span class="fu">&lt;$&gt;</span> randomWeights i o
randomNet i (h<span class="fu">:</span>hs) o <span class="fu">=</span> (<span class="fu">:&amp;~</span>) <span class="fu">&lt;$&gt;</span> randomWeights i h <span class="fu">&lt;*&gt;</span> randomNet h hs o</code></pre></div>
<p>(<code>randomVector</code> and <code>uniformSample</code> are from the <em>hmatrix</em> library, generating random vectors and matrices from a random <code>Int</code> seed. We configure them to generate them with numbers between -1 and 1)</p>
<p>And now a function to “run” our network on a given input vector:</p>
<div class="sourceCode"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span class="co">-- source: https://github.com/mstksg/inCode/tree/master/code-samples/dependent-haskell/NetworkUntyped.hs#L20-34</span>
<span class="ot">logistic ::</span> <span class="dt">Double</span> <span class="ot">-&gt;</span> <span class="dt">Double</span>
logistic x <span class="fu">=</span> <span class="dv">1</span> <span class="fu">/</span> (<span class="dv">1</span> <span class="fu">+</span> exp (<span class="fu">-</span>x))

<span class="ot">runLayer ::</span> <span class="dt">Weights</span> <span class="ot">-&gt;</span> <span class="dt">Vector</span> <span class="dt">Double</span> <span class="ot">-&gt;</span> <span class="dt">Vector</span> <span class="dt">Double</span>
runLayer (<span class="dt">W</span> wB wW) v <span class="fu">=</span> wB <span class="fu">+</span> (wW <span class="fu">#&gt;</span> v)

<span class="ot">runNet ::</span> <span class="dt">Network</span> <span class="ot">-&gt;</span> <span class="dt">Vector</span> <span class="dt">Double</span> <span class="ot">-&gt;</span> <span class="dt">Vector</span> <span class="dt">Double</span>
runNet (<span class="dt">O</span> w)      <span class="fu">!</span>v <span class="fu">=</span> logistic <span class="ot">`cmap`</span> runLayer w v
runNet (w <span class="fu">:&amp;~</span> n&#39;) <span class="fu">!</span>v <span class="fu">=</span> <span class="kw">let</span> v&#39; <span class="fu">=</span> logistic <span class="ot">`cmap`</span> runLayer w v
                       <span class="kw">in</span>  runNet n&#39; v&#39;</code></pre></div>
<p>(<code>#&gt;</code> is matrix-vector multiplication)</p>
<p>TODO: examples of running</p>
<p>If you’re a normal programmer, this might seem perfectly fine. If you are a Haskell programmer, you should already be having heart attacks. Let’s imagine all of the bad things that could happen:</p>
<ul>
<li><p>How do we even know that each subsequent matrix in the network is “compatible”? We want the outputs of one matrix to line up with the inputs of the next, but there’s no way to know unless we have “smart constructors” to check while we add things. But it’s possible to build a bad network, and things will just explode at runtime.</p></li>
<li><p>How do we know the size vector the network expects? What stops you from sending in a bad vector at run-time and having everything explode?</p></li>
<li><p>How do we verify that we have implemented <code>runLayer</code> and <code>runNet</code> in a way that they won’t suddenly fail at runtime? We write <code>l #&gt; v</code>, but how do we know that it’s even correct? We can it prove ourselves, but the compiler won’t help us.</p></li>
</ul>
<p>Now, let’s try implementing back-propagation:</p>
<div class="sourceCode"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span class="ot">train ::</span> <span class="dt">Vector</span> <span class="dt">Double</span> <span class="ot">-&gt;</span> <span class="dt">Vector</span> <span class="dt">Double</span> <span class="ot">-&gt;</span> <span class="dt">Network</span> <span class="ot">-&gt;</span> <span class="dt">Network</span>
train i o <span class="fu">=</span> go i
  <span class="kw">where</span>
<span class="ot">    go ::</span> <span class="dt">Vector</span> <span class="dt">Double</span> <span class="ot">-&gt;</span> <span class="dt">Network</span> <span class="ot">-&gt;</span> (<span class="dt">Vector</span> <span class="dt">Double</span>, <span class="dt">Network</span>)
    go <span class="fu">=</span> undefined</code></pre></div></div><footer><ul class="entry-series"></ul><ul class="tag-list"><li><a href="http://blog.jle.im/entries/tagged/functional-programming.html" class="tag-a-tag">#functional programming</a></li><li><a href="http://blog.jle.im/entries/tagged/haskell.html" class="tag-a-tag">#haskell</a></li><li><a href="http://blog.jle.im/entries/tagged/types.html" class="tag-a-tag">#types</a></li><li><a href="http://blog.jle.im/entries/category/@haskell.html" class="tag-a-category">@HASKELL</a></li><li><a href="http://blog.jle.im/entries/category/@ramblings.html" class="tag-a-category">@RAMBLINGS</a></li></ul><aside class="social-buttons"><div class="addthis_toolbox addthis_default_style addthis-buttons"><a class="addthis_button_facebook_like" fb:like:layout="button_count"></a><a class="addthis_button_tweet"></a><a class="addthis_button_google_plusone" g:plusone:size="medium"></a><a class="addthis_counter addthis_pill_style"></a></div><div class="custom-social-buttons"><div class="custom-social-button"><a href="http://www.reddit.com/submit" onclick="window.location = &#39;http://www.reddit.com/submit?url=&#39;+ encodeURIComponent(window.location); return false"><img src="http://www.reddit.com/static/spreddit7.gif" alt="submit to reddit"></a></div></div></aside><nav class="next-prev-links"><ul><li class="next-entry-link">(Next) <a href="http://blog.jle.im/entry/introducing-in-code.html">Introducing “in Code”!</a> &rarr;</li></ul></nav></footer></article><div class="post-entry"><div class="tile"><div id="disqus_thread"></div><noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a><br></noscript><a href="http://disqus.com" class="dsq-brlink">Comments powered by <span class="logo-disqus">Disqus</span></a></div></div></div></div></div><div id="footer-container"><div id="footer-content"><div class="tile"><div class="footer-copyright">&copy; 2016 Justin Le</div><div class="footer-follow social-follows"><ul class="social-follows-list"><li><ul class="social-follows-list-social"><li><a class="social-follow-facebook" title="Friend me on Facebook!" href="https://facebook.com/mstksg">Facebook</a></li><li><a class="social-follow-twitter" title="Follow me on Twitter!" href="https://twitter.com/intent/user?user_id=mstk" onclick="window.open(
  &#39;http://twitter.com/intent/user?user_id=907281&#39;,
  &#39;facebook-share-dialog&#39;,
  &#39;width=550,height=520&#39;);
return false;
">Twitter</a></li><li><a class="social-follow-gplus" title="Add me on Google+!" href="https://plus.google.com/+JustinLe">Google+</a></li><li><a class="social-follow-linkedin" title="Connect with me on LinkedIn!" href="https://linkedin.com/in/mstksg">LinkedIn</a></li><li><a class="social-follow-github" title="Fork me on Github!" href="https://github.com/lejustin">Github</a></li><li><a class="social-follow-keybase" title="Track me on Keybase!" href="https://keybase.io/mstksg">Keybase</a></li><li><a class="social-follow-bitcoin" title="Donate via bitcoin!" href="https://coinbase.com/mstksg">Bitcoin</a></li></ul></li><li><ul class="social-follows-list-site"><li><a class="social-follow-rss" title="Subscribe to my RSS Feed!" href="http://feeds.feedburner.com/incodeblog">RSS</a></li><li><a class="social-follow-email" title="Subscribe to the mailing list!" href="https://feedburner.google.com/fb/a/mailverify?loc=en_US&amp;uri=incodeblog">Mailing list</a></li></ul></li></ul></div></div></div></div></body></html>