<!DOCTYPE HTML>
<html><head><title>Practical Dependent Types in Haskell: Type-Safe Neural Networks · in Code</title><meta name="description" content="Weblog of Justin Le, covering his various adventures in programming and explorations in the vast worlds of computation physics, and knowledge."><meta http-equiv="Content-Type" content="text/html;charset=utf-8"><meta name="viewport" content="width=device-width,initial-scale=1.0"><meta property="og:site_name" content="in Code"><meta property="og:description" content="Whether you like it or not, programming with dependent types in Haskell moving slowly but steadily to the mainstream of Haskell programming. In the current state of Haskell education, dependent types are often considered topics for advanced Haskell users. However, I can definitely foresee a day where the ease of use of modern Haskell libraries relying on dependent types as well as their ubiquitousness forces programming with dependent types to be an integral part of regular intermediate (or even beginner) Haskell education, as much as Traversable or Maps. The point of this post is to show some practical examples of using dependent types in the real world, and to also walk through the why and high-level philosophy of the way you structure your Haskell programs. It’ll also hopefully instill an intuition of a dependently typed work flow of exploring how dependent types can help your current programs. The first project in this series will build up to type-safe artificial neural network implementations. Hooray! &lt;!-- There are other great tutorials I&#39;d recommend online if you want to explore --&gt; &lt;!-- dependent types in Haskell further, including [this great servant --&gt; &lt;!-- &quot;tutorial&quot;][servtut]. Also, I should provide a disclaimer --- I&#39;m also --&gt;"><meta property="og:type" content="article"><meta property="og:title" content="Practical Dependent Types in Haskell: Type-Safe Neural Networks"><meta property="og:image" content="https://blog.jle.im/img/site_logo.jpg"><meta property="og:locale" content="en_US"><meta property="og:url" content="https://blog.jle.im/entry/practical-dependent-types-in-haskell-1.html"><meta name="twitter:card" content="summary"><meta name="twitter:creator:id" content="mstk"><link rel="author" href="https://plus.google.com/107705320197444500140"><link rel="alternate" type="application/rss+xml" title="in Code (RSS Feed)" href="http://feeds.feedburner.com/incodeblog"><link rel="canonical" href="https://blog.jle.im/entry/practical-dependent-types-in-haskell-1.html"><link href="https://blog.jle.im/favicon.ico" rel="shortcut icon"><link href="https://blog.jle.im/css/toast.css" rel="stylesheet" type="text/css"><link href="https://blog.jle.im/css/font.css" rel="stylesheet" type="text/css"><link href="https://blog.jle.im/css/main.css" rel="stylesheet" type="text/css"><link href="https://blog.jle.im/css/page/entry.css" rel="stylesheet" type="text/css"><link href="https://blog.jle.im/css/pygments.css" rel="stylesheet" type="text/css"><script type="text/javascript">var page_data = {};
var disqus_shortname='incode';
</script><script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');
ga('create', 'UA-443711-8', 'jle.im');
ga('send', 'pageview');
</script><script type="text/javascript" src="//ajax.googleapis.com/ajax/libs/jquery/1.10.2/jquery.min.js"></script><script type="text/javascript" src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5234d67a6b68dcd4"></script><script type="text/javascript" src="https://blog.jle.im/js/page/entry_toc.js"></script><script type="text/javascript" src="https://blog.jle.im/js/disqus_count.js"></script><script type="text/javascript" src="https://blog.jle.im/js/social.js"></script><script type="text/javascript" src="https://blog.jle.im/js/jquery/jquery.toc.js"></script><script type="text/javascript" src="https://blog.jle.im/purescript/entry.js"></script></head><body><div id="fb-root"><script>(function(d, s, id) {
 var js, fjs = d.getElementsByTagName(s)[0];
 if (d.getElementById(id)) return;
 js = d.createElement(s); js.id = id;
 js.src = "//connect.facebook.net/en_US/all.js#xfbml=1&appId=641852699171929";
 fjs.parentNode.insertBefore(js, fjs);
}(document, 'script', 'facebook-jssdk'));
</script></div><div id="header-container"><div id="navbar-container" class="tile"><nav id="navbar-content"><div class="nav-info"><h1 class="site-title"><a href="https://blog.jle.im/" class="nav-title">in Code</a></h1><span class="nav-author">Justin Le</span></div><ul class="nav-links"><li><a href="https://blog.jle.im/">home</a></li><li><a href="https://blog.jle.im/entries.html">archives</a></li><div class="clear"></div></ul></nav></div><div id="header-content"></div></div><div id="body-container" class="container"><div id="main-container" class="grid"><div class="entry-section unit span-grid" role="main"><article class="tile article"><header><div class="unposted-banner">Unposted entry</div><h1 id="title">Practical Dependent Types in Haskell: Type-Safe Neural Networks</h1><p class="entry-info">by <a class="author" href="https://blog.jle.im/">Justin Le</a></p><p><span class="source-info"><a class="source-link" href="https://github.com/mstksg/inCode/tree/master/copy/entries/dependent-haskell-1.md">Source</a><span class="info-separator"> &diams; </span><a class="source-link" href="https://blog.jle.im/entry/practical-dependent-types-in-haskell-1.md">Markdown</a><span class="info-separator"> &diams; </span><a class="source-link" href="https://blog.jle.im/entry/practical-dependent-types-in-haskell-1.tex">LaTeX</a><span class="info-separator"> &diams; </span></span>Posted in <a href="https://blog.jle.im/entries/category/@haskell.html" class="tag-a-category" title="Functional, pure, non-strict, statically and strongly typed, natively
compiled…really just the king of great languages.">Haskell</a>, <a href="https://blog.jle.im/entries/category/@ramblings.html" class="tag-a-category" title="My slight ramblings on subjects of interest (to me and hopefully to you
too!). Lots of surveys and introducts to new subjects.">Ramblings</a><span class="info-separator"> &diams; </span><a class="comment-link" href="#disqus_thread">Comments</a></p></header><hr><aside class="contents-container"><h5 id="contents-header">Contents</h5><div id="toc"></div></aside><div class="main-content copy-content"><p>Whether you like it or not, programming with dependent types in Haskell moving slowly but steadily to the mainstream of Haskell programming. In the current state of Haskell education, dependent types are often considered topics for “advanced” Haskell users. However, I can definitely foresee a day where the ease of use of modern Haskell libraries relying on dependent types as well as their ubiquitousness forces programming with dependent types to be an integral part of regular intermediate (or even beginner) Haskell education, as much as Traversable or Maps.</p>
<p>The point of this post is to show some practical examples of using dependent types in the real world, and to also walk through the “why” and high-level philosophy of the way you structure your Haskell programs. It’ll also hopefully instill an intuition of a dependently typed work flow of “exploring” how dependent types can help your current programs.</p>
<p>The first project in this series will build up to type-safe <strong><a href="https://en.wikipedia.org/wiki/Artificial_neural_network">artificial neural network</a></strong> implementations. Hooray!</p>
<!-- There are other great tutorials I'd recommend online if you want to explore -->
<!-- dependent types in Haskell further, including [this great servant -->
<!-- "tutorial"][servtut].  Also, I should provide a disclaimer --- I'm also -->
<!-- currently exploring all of this as I'm going along too. It's a wild world out -->
<!-- there.  Join me and let's be a part of the frontier! -->
<!-- [servtut]: http://www.well-typed.com/blog/2015/11/implementing-a-minimal-version-of-haskell-servant/ -->
<h2 id="neural-networks">Neural Networks</h2>
<p><a href="https://en.wikipedia.org/wiki/Artificial_neural_network">Artificial neural networks</a> have been somewhat of a hot topic in computing recently. At their core they involve matrix multiplication and manipulation, so they do seem like a good candidate for a dependent types. Most importantly, implementations of training algorithms (like back-propagation) are tricky to implement correctly — despite being simple, there are many locations where accidental bugs might pop up when multiplying the wrong matrices, for example.</p>
<p>However, it’s not always easy to gauge before-the-fact what would or would not be a good candidate for adding dependent types to, and often times, it can be considered premature to start off with “as powerful types as you can”. So let’s walk through programming things with as “dumb” types as possible, and see where types can help.</p>
<p>Edwin Brady calls this process “type-driven development”. Start general, recognize the partial functions and red flags, and slowly add more powerful types.</p>
<h3 id="the-network">The Network</h3>
<figure>
<img src="/img/entries/dependent-haskell-1/ffneural.png" title="Feed-forward ANN architecture" alt="Feed-forward ANN architecture" /><figcaption>Feed-forward ANN architecture</figcaption>
</figure>
<p>We’re going to be implementing a feed-forward neural network, with back-propagation training. These networks are layers of “nodes”, each connected to the each of the nodes of the previous layer.</p>
<p>Input goes to the first layer, which feeds information to the next year, which feeds it to the next, etc., until the final layer, where we read it off as the “answer” that the network is giving us. Layers between the input and output layers are called <em>hidden</em> layers. Every node “outputs” a weighted sum of all of the outputs of the <em>previous</em> layer, plus an always-on “bias” term (so that its result can be non-zero even when all of its inputs are zero). Symbolically, it looks like:</p>
<p><br /><img style="vertical-align:middle" src="https://latex.codecogs.com/gif.latex?%0Ay_j%20%3D%20b_j%20%2B%20%5Csum_i%5Em%20w_%7Bij%7D%20x_i%0A" alt="
y_j = b_j + \sum_i^m w_{ij} x_i
" title="
y_j = b_j + \sum_i^m w_{ij} x_i
" /><br /></p>
<p>Or, if we treat the output of a layer and the list of list of weights as a matrix, we can write it a little cleaner:</p>
<p><br /><img style="vertical-align:middle" src="https://latex.codecogs.com/gif.latex?%0A%5Cmathbf%7By%7D%20%3D%20%5Cmathbf%7Bb%7D%20%2B%20W%20%5Cmathbf%7Bx%7D%0A" alt="
\mathbf{y} = \mathbf{b} + W \mathbf{x}
" title="
\mathbf{y} = \mathbf{b} + W \mathbf{x}
" /><br /></p>
<p>To “scale” the result (and to give the system the magical powers of nonlinearity), we actually apply an “activation function” to the output before passing it down to the next step. We’ll be using the popular <a href="https://en.wikipedia.org/wiki/Logistic_function">logistic function</a>, <img style="vertical-align:middle" src="https://latex.codecogs.com/gif.latex?f%28x%29%20%3D%201%20%2F%20%281%20%2B%20e%5E%7B-x%7D%29" alt="f(x) = 1 / (1 + e^{-x})" title="f(x) = 1 / (1 + e^{-x})" />.</p>
<p><em>Training</em> a network involves picking the right set of weights to get the network to answer the question you want.</p>
<h2 id="vanilla-types">Vanilla Types</h2>
<p>We can store a network by storing the matrix of of weights and biases between each layer:</p>
<div class="sourceCode"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span class="co">-- source: https://github.com/mstksg/inCode/tree/master/code-samples/dependent-haskell/NetworkUntyped.hs#L17-19</span>
<span class="kw">data</span> <span class="dt">Weights</span> <span class="fu">=</span> <span class="dt">W</span> {<span class="ot"> wBiases ::</span> <span class="fu">!</span>(<span class="dt">Vector</span> <span class="dt">Double</span>)  <span class="co">-- m</span>
                 ,<span class="ot"> wNodes  ::</span> <span class="fu">!</span>(<span class="dt">Matrix</span> <span class="dt">Double</span>)  <span class="co">-- m x n</span>
                 }                              <span class="co">-- &quot;n to m&quot; layer</span></code></pre></div>
<p>Now, a <code>Weights</code> linking an <em>n</em>-node layer to an <em>m</em>-node layer has an <em>m</em>-dimensional bias vector (one component for each output) and an <em>m</em>-by-<em>n</em> node weight matrix (one column for each output, one row for each input).</p>
<p>(We’re using the <code>Matrix</code> type from the awesome <em><a href="http://hackage.haskell.org/package/hmatrix">hmatrix</a></em> library for performant linear algebra, implemented using blas/lapack under the hood)</p>
<p>A feed-forward neural network is then just a linked list of these weights:</p>
<div class="sourceCode"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span class="co">-- source: https://github.com/mstksg/inCode/tree/master/code-samples/dependent-haskell/NetworkUntyped.hs#L21-27</span>
<span class="kw">data</span> <span class="dt">Network</span><span class="ot"> ::</span> <span class="fu">*</span> <span class="kw">where</span>
    <span class="dt">O</span><span class="ot">     ::</span> <span class="fu">!</span><span class="dt">Weights</span>
          <span class="ot">-&gt;</span> <span class="dt">Network</span>
<span class="ot">    (:&amp;~) ::</span> <span class="fu">!</span><span class="dt">Weights</span>
          <span class="ot">-&gt;</span> <span class="fu">!</span><span class="dt">Network</span>
          <span class="ot">-&gt;</span> <span class="dt">Network</span>
<span class="kw">infixr</span> <span class="dv">5</span> <span class="fu">:&amp;~</span></code></pre></div>
<p>Note that we’re using <a href="https://en.wikibooks.org/wiki/Haskell/GADT">GADT</a> syntax here, which just lets us define <code>Network</code> by providing the type of its <em>constructors</em>, <code>O</code> and <code>(:&amp;~)</code>. A network with one input layer, two inner layers, and one output layer would look like:</p>
<div class="sourceCode"><pre class="sourceCode haskell"><code class="sourceCode haskell">ih <span class="fu">:&amp;~</span> hh <span class="fu">:&amp;~</span> <span class="dt">O</span> ho</code></pre></div>
<p>The first component is the weights from the input to first inner layer, the second is the weights between the two hidden layers, and the last is the weights between the last hidden layer and the output layer.</p>
<!-- TODO: graphs using diagrams? -->
<p>We can write simple procedures, like generating random networks:</p>
<div class="sourceCode"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span class="co">-- source: https://github.com/mstksg/inCode/tree/master/code-samples/dependent-haskell/NetworkUntyped.hs#L45-55</span>
<span class="ot">randomWeights ::</span> <span class="dt">MonadRandom</span> m <span class="ot">=&gt;</span> <span class="dt">Int</span> <span class="ot">-&gt;</span> <span class="dt">Int</span> <span class="ot">-&gt;</span> m <span class="dt">Weights</span>
randomWeights i o <span class="fu">=</span> <span class="kw">do</span>
    s1 <span class="ot">&lt;-</span> getRandom
    s2 <span class="ot">&lt;-</span> getRandom
    <span class="kw">let</span> wB <span class="fu">=</span> randomVector s1 <span class="dt">Uniform</span> o <span class="fu">*</span> <span class="dv">2</span> <span class="fu">-</span> <span class="dv">1</span>
        wN <span class="fu">=</span> uniformSample s2 o (replicate i (<span class="fu">-</span><span class="dv">1</span>, <span class="dv">1</span>))
    return <span class="fu">$</span> <span class="dt">W</span> wB wN

<span class="ot">randomNet ::</span> <span class="dt">MonadRandom</span> m <span class="ot">=&gt;</span> <span class="dt">Int</span> <span class="ot">-&gt;</span> [<span class="dt">Int</span>] <span class="ot">-&gt;</span> <span class="dt">Int</span> <span class="ot">-&gt;</span> m <span class="dt">Network</span>
randomNet i [] o     <span class="fu">=</span>     <span class="dt">O</span> <span class="fu">&lt;$&gt;</span> randomWeights i o
randomNet i (h<span class="fu">:</span>hs) o <span class="fu">=</span> (<span class="fu">:&amp;~</span>) <span class="fu">&lt;$&gt;</span> randomWeights i h <span class="fu">&lt;*&gt;</span> randomNet h hs o</code></pre></div>
<p>(<code>randomVector</code> and <code>uniformSample</code> are from the <em>hmatrix</em> library, generating random vectors and matrices from a random <code>Int</code> seed. We configure them to generate them with numbers between -1 and 1)</p>
<p>And now a function to “run” our network on a given input vector, following the matrix equation we wrote earlier:</p>
<div class="sourceCode"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span class="co">-- source: https://github.com/mstksg/inCode/tree/master/code-samples/dependent-haskell/NetworkUntyped.hs#L29-43</span>
<span class="ot">logistic ::</span> <span class="dt">Floating</span> a <span class="ot">=&gt;</span> a <span class="ot">-&gt;</span> a
logistic x <span class="fu">=</span> <span class="dv">1</span> <span class="fu">/</span> (<span class="dv">1</span> <span class="fu">+</span> exp (<span class="fu">-</span>x))

<span class="ot">runLayer ::</span> <span class="dt">Weights</span> <span class="ot">-&gt;</span> <span class="dt">Vector</span> <span class="dt">Double</span> <span class="ot">-&gt;</span> <span class="dt">Vector</span> <span class="dt">Double</span>
runLayer (<span class="dt">W</span> wB wN) v <span class="fu">=</span> wB <span class="fu">+</span> wN <span class="fu">#&gt;</span> v

<span class="ot">runNet ::</span> <span class="dt">Network</span> <span class="ot">-&gt;</span> <span class="dt">Vector</span> <span class="dt">Double</span> <span class="ot">-&gt;</span> <span class="dt">Vector</span> <span class="dt">Double</span>
runNet (<span class="dt">O</span> w)      <span class="fu">!</span>v <span class="fu">=</span> logistic (runLayer w v)
runNet (w <span class="fu">:&amp;~</span> n&#39;) <span class="fu">!</span>v <span class="fu">=</span> <span class="kw">let</span> v&#39; <span class="fu">=</span> logistic (runLayer w v)
                       <span class="kw">in</span>  runNet n&#39; v&#39;</code></pre></div>
<p>(<code>#&gt;</code> is matrix-vector multiplication)</p>
<!-- TODO: examples of running -->
<p>If you’re a normal programmer, this might seem perfectly fine. If you are a Haskell programmer, you should already be having heart attacks. Let’s imagine all of the bad things that could happen:</p>
<ul>
<li><p>How do we even know that each subsequent matrix in the network is “compatible”? We want the outputs of one matrix to line up with the inputs of the next, but there’s no way to know unless we have “smart constructors” to check while we add things. But it’s possible to build a bad network, and things will just explode at runtime.</p></li>
<li><p>How do we know the size vector the network expects? What stops you from sending in a bad vector at run-time?</p></li>
<li><p>How do we verify that we have implemented <code>runLayer</code> and <code>runNet</code> in a way that they won’t suddenly fail at runtime? We write <code>l #&gt; v</code>, but how do we know that it’s even correct…what if we forgot to multiply something, or used something in the wrong places? We can it prove ourselves, but the compiler won’t help us.</p></li>
</ul>
<h3 id="back-propagation">Back-propagation</h3>
<p>Now, let’s try implementing back-propagation! It’s a basic “gradient descent” algorithm. There are <a href="https://en.wikipedia.org/wiki/Backpropagation">many explanations</a> on the internet; the basic idea is that you try to minimize the squared “error” of what the neural network outputs for a given input vs. the actual expected output. You find the direction of change that minimizes the error, and move that direction. The implementation of Feed-forward backpropagation is found in many sources online and in literature, so let’s see the implementation in Haskell:</p>
<div class="sourceCode"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span class="co">-- source: https://github.com/mstksg/inCode/tree/master/code-samples/dependent-haskell/NetworkUntyped.hs#L57-85</span>
<span class="ot">train ::</span> <span class="dt">Double</span>           <span class="co">-- ^ learning rate</span>
      <span class="ot">-&gt;</span> <span class="dt">Vector</span> <span class="dt">Double</span>    <span class="co">-- ^ input vector</span>
      <span class="ot">-&gt;</span> <span class="dt">Vector</span> <span class="dt">Double</span>    <span class="co">-- ^ target vector</span>
      <span class="ot">-&gt;</span> <span class="dt">Network</span>          <span class="co">-- ^ network to train</span>
      <span class="ot">-&gt;</span> <span class="dt">Network</span>
train rate x0 target <span class="fu">=</span> fst <span class="fu">.</span> go x0
  <span class="kw">where</span>
<span class="ot">    go  ::</span> <span class="dt">Vector</span> <span class="dt">Double</span>    <span class="co">-- ^ input vector</span>
        <span class="ot">-&gt;</span> <span class="dt">Network</span>          <span class="co">-- ^ network to train</span>
        <span class="ot">-&gt;</span> (<span class="dt">Network</span>, <span class="dt">Vector</span> <span class="dt">Double</span>)
    go <span class="fu">!</span>x (<span class="dt">O</span> w<span class="fu">@</span>(<span class="dt">W</span> wB wN))
        <span class="fu">=</span> <span class="kw">let</span> y    <span class="fu">=</span> runLayer w x
              o    <span class="fu">=</span> logistic y
              dEdy <span class="fu">=</span> logistic&#39; y <span class="fu">*</span> (o <span class="fu">-</span> target)
              wB&#39;  <span class="fu">=</span> wB <span class="fu">-</span> scale rate dEdy
              wN&#39;  <span class="fu">=</span> wN <span class="fu">-</span> scale rate (dEdy <span class="ot">`outer`</span> x)
              w&#39;   <span class="fu">=</span> <span class="dt">W</span> wB&#39; wN&#39;
              dWs  <span class="fu">=</span> tr wN <span class="fu">#&gt;</span> dEdy
          <span class="kw">in</span>  (<span class="dt">O</span> w&#39;, dWs)
    go <span class="fu">!</span>x (w<span class="fu">@</span>(<span class="dt">W</span> wB wN) <span class="fu">:&amp;~</span> n)
        <span class="fu">=</span> <span class="kw">let</span> y          <span class="fu">=</span> runLayer w x
              o          <span class="fu">=</span> logistic y
              (n&#39;, dWs&#39;) <span class="fu">=</span> go o n
              dEdy       <span class="fu">=</span> logistic&#39; y <span class="fu">*</span> dWs&#39;
              wB&#39;        <span class="fu">=</span> wB <span class="fu">-</span> scale rate dEdy
              wN&#39;        <span class="fu">=</span> wN <span class="fu">-</span> scale rate (dEdy <span class="ot">`outer`</span> x)
              w&#39;         <span class="fu">=</span> <span class="dt">W</span> wB&#39; wN&#39;
              dWs        <span class="fu">=</span> tr wN <span class="fu">#&gt;</span> dEdy
          <span class="kw">in</span>  (w&#39; <span class="fu">:&amp;~</span> n&#39;, dWs)</code></pre></div>
<p>Where <code>logistic'</code> is the derivative of <code>logistic</code>. The algorithm computes the <em>updated</em> network by recursively updating the layers, from the output layer all the way up to the input layer. At every step it returns the updated layer/network, as well as a bundle of derivatives for the next layer to use to calculate its descent direction. At the output layer, all it needs to calculate the direction of descent is just <code>o - targ</code>, the target. At the inner layers, it has to use the <code>dWs</code> bundle to figure it out.</p>
<p>Writing this is a bit of a struggle. The type system doesn’t help you like it normally does in Haskell, and you can’t really use parametricity to help you write your code like normal Haskell. Everything is monomorphic, and everything multiplies with everything else. You don’t have any hits about what to multiply with what at any point in time. Seeing the implementation here basically amplifies and puts on displays all of the red flags/awfulness mentioned before.</p>
<p>In short, you’re leaving yourself open to many potential bugs…and the compiler doesn’t help you write your code at all! This is the nightmare of every Haskell programmer. There must be a better way!</p>
<h4 id="tests">Tests</h4>
<p>Pretty much the only way you can verify this code is to test it out on example cases. In the <a href="https://github.com/mstksg/inCode/tree/master/code-samples/dependent-haskell/NetworkUntyped.hs">source file</a> I have <code>main</code> test out the backprop, training a network on a 2D function that was “on” for two small circles and “off” everywhere else (A nice cute non-linearly-separable function to test our network on). We basically train the network to be able to recognize the two-circle pattern. I implemented a simple printing function and tested the trained network on a grid:</p>
<div class="sourceCode"><pre class="sourceCode bash"><code class="sourceCode bash">$ <span class="kw">stack</span> install hmatrix MonadRandom
$ <span class="kw">stack</span> ghc -- -O2 ./NetworkUntyped.hs
$ <span class="kw">./NetworkUntyped.hs</span>
<span class="co"># Training network...</span>
<span class="co">#</span>
<span class="co">#</span>
<span class="co">#            .=########=</span>
<span class="co">#          .##############.</span>
<span class="co">#          ################</span>
<span class="co">#          ################</span>
<span class="co">#          .##############-</span>
<span class="co">#            .###########</span>
<span class="co">#                 ...             ...</span>
<span class="co">#                             -##########.</span>
<span class="co">#                           -##############.</span>
<span class="co">#                           ################</span>
<span class="co">#                           ################</span>
<span class="co">#                            =############=</span>
<span class="co">#                              .#######=.</span>
<span class="co">#</span>
<span class="co">#</span></code></pre></div>
<p>Not too bad! But, I was basically forced to resort to unit testing to ensure my code was correct. Let’s see if we can do better.</p>
<h3 id="the-call-of-types">The Call of Types</h3>
<p>Before we go on to the “typed” version of our program, let’s take a step back and look at some big checks you might want to ask yourself after you write code in Haskell.</p>
<ol type="1">
<li>Are any of my functions partial, or implemented using partial functions?</li>
<li>How could I have written things that are <em>incorrect</em>, and yet still type check? Where does the compiler <em>not</em> help me by restricting my choices?</li>
</ol>
<p>Both of these questions usually yield some truth about the code you write and the things you should worry about. As a Haskeller, they should always be at the back of your mind!</p>
<p>Looking back at our untyped implementation, we notice some things:</p>
<ol type="1">
<li>Almost every single function we wrote is partial. If we had passed in the incorrectly sized matrix/vector, or stored mismatched vectors in our network, everything would fall apart.</li>
<li>There are literally billions of ways we could have implemented our functions where they would still typechecked. We could multiply mismatched matrices, or forget to multiply a matrix, etc.</li>
</ol>
<h2 id="with-static-types">With Static Types</h2>
<p>Gauging our potential problems, it seems like the first major class of bugs we can address is improperly sized and incompatible matrices. If the compiler always made sure we used compatible matrices, we can avoid bugs at compile-time, and we also can get a friendly helper when we write programs.</p>
<p>Let’s write a <code>Weights</code> type that tells you the size of its output and the input it expects. Let’s have, say, a <code>Weights 10 5</code> be a set of weights that takes you from a layer of 10 nodes to a layer of 5 nodes. <code>w : Weights 4 6</code> would take you from a layer of 4 nodes to a layer of 6 nodes:</p>
<div class="sourceCode"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span class="co">-- source: https://github.com/mstksg/inCode/tree/master/code-samples/dependent-haskell/NetworkTyped.hs#L22-24</span>
<span class="kw">data</span> <span class="dt">Weights</span> i o <span class="fu">=</span> <span class="dt">W</span> {<span class="ot"> wBiases ::</span> <span class="fu">!</span>(<span class="dt">R</span> o)
                     ,<span class="ot"> wNodes  ::</span> <span class="fu">!</span>(<span class="dt">L</span> o i)
                     }</code></pre></div>
<p>We’re using the <code>Numeric.LinearAlgebra.Static</code> module from <em><a href="http://hackage.haskell.org/package/hmatrix">hmatrix</a></em>, which offers matrix and vector types with their size in their types: an <code>R 5</code> is a vector of Doubles with 5 elements, and a <code>L 3 6</code> is a 3x6 vector of Doubles.</p>
<p>The <code>Static</code> module relies on the <code>KnownNat</code> mechanism that GHC offers. A <code>KnownNat n</code> constraint is pretty much a way for you to “get” the value at runtime, so a <code>KnownNat n =&gt; R n</code> is basically a vector “packaged” with its size via <code>KnownNat n</code>. Almost all operations in the library require a <code>KnownNat</code> constraint.</p>
<p>A reasonable type for a network might be <code>Network 10 2</code>, taking 10 inputs and popping out 2 outputs. This might be an ideal type to export, because it abstracts away the size of the hidden layers. But it’d be nice for us to keep all of the hidden layers in the type for now — we’ll see how it can be useful, and we’ll also talk about how to later hide/abstract it away when we export the type.</p>
<p>Our network type can be something like <code>Network 10 '[7,5,3] 2</code>: Take 10 inputs, return 2 outputs. And internally, have hidden layers of size 7, 5, and 3. (The <code>'[7,5,3]</code> is a type-level list of Nats; the optional <code>'</code> apostrophe is just for our own benefit to distinguish it from a value-level list of integers.)</p>
<div class="sourceCode"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span class="co">-- source: https://github.com/mstksg/inCode/tree/master/code-samples/dependent-haskell/NetworkTyped.hs#L26-33</span>
<span class="kw">data</span> <span class="dt">Network</span><span class="ot"> ::</span> <span class="dt">Nat</span> <span class="ot">-&gt;</span> [<span class="dt">Nat</span>] <span class="ot">-&gt;</span> <span class="dt">Nat</span> <span class="ot">-&gt;</span> <span class="fu">*</span> <span class="kw">where</span>
    <span class="dt">O</span><span class="ot">     ::</span> <span class="fu">!</span>(<span class="dt">Weights</span> i o)
          <span class="ot">-&gt;</span> <span class="dt">Network</span> i <span class="ch">&#39;[] o</span>
<span class="ot">    (:&amp;~) ::</span> <span class="dt">KnownNat</span> h
          <span class="ot">=&gt;</span> <span class="fu">!</span>(<span class="dt">Weights</span> i h)
          <span class="ot">-&gt;</span> <span class="fu">!</span>(<span class="dt">Network</span> h hs o)
          <span class="ot">-&gt;</span> <span class="dt">Network</span> i (h <span class="ch">&#39;: hs) o</span>
<span class="kw">infixr</span> <span class="dv">5</span> <span class="fu">:&amp;~</span></code></pre></div>
<p>We use GADT syntax here again, but let’s go over the two constructors.</p>
<ul>
<li><p>The <code>O</code> constructor takes a <code>Weights i o</code> and returns a <code>Network i '[] o</code>. That is, if your network is just weights from <code>i</code> inputs to <code>o</code> outputs, your network itself just takes <code>i</code> inputs and returns <code>o</code> outputs.</p></li>
<li><p>The <code>(:&amp;~)</code> constructor takes a <code>Network h hs o</code> – a network with <code>h</code> inputs and <code>o</code> outputs – and “conses” an extra input layer in front. If you give it a <code>Weights i h</code>, its outputs fit perfectly into the inputs of the subnetwork, and you get a <code>Network i hs o</code>.</p>
<p>We add a <code>KnownNat</code> constraint on the <code>h</code>, so that whenever you pattern match on <code>w :&amp;~ net</code>, you automatically get a <code>KnownNat</code> constraint for the input size of <code>net</code> that you can use.</p></li>
</ul>
<p>We can still construct them the same way:</p>
<div class="sourceCode"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span class="ot">ho ::</span> <span class="dt">Weights</span>  <span class="dv">4</span> <span class="dv">2</span>
<span class="ot">hh ::</span> <span class="dt">Weights</span>  <span class="dv">7</span> <span class="dv">4</span>
<span class="ot">ih ::</span> <span class="dt">Weights</span> <span class="dv">10</span> <span class="dv">7</span>

<span class="dt">O</span><span class="ot"> ho                    ::</span> <span class="dt">Network</span>  <span class="dv">4</span> <span class="ch">&#39;[] 2</span>
hh <span class="fu">:&amp;~</span> <span class="dt">O</span><span class="ot"> ho             ::</span> <span class="dt">Network</span>  <span class="dv">7</span> <span class="ch">&#39;[4] 2</span>
ih <span class="fu">:&amp;~</span> hh <span class="fu">:&amp;~</span> <span class="dt">O</span><span class="ot"> ho      ::</span> <span class="dt">Network</span> <span class="dv">10</span> <span class="ch">&#39;[7,4] 2</span></code></pre></div>
<p>Note that the shape of the constructors requires all of the weight vectors to “fit together” Now if we ever pattern match on <code>:&amp;~</code>, we know that the resulting matrices and vectors are compatible!</p>
<p>Note that this approach is also self-documenting. I don’t need to specify what the dimensions are in the docs and trust the users to read it. The types tell them! And if they don’t listen, they get a compiler error!</p>
<p>Generating random weights and networks is even nicer now:</p>
<div class="sourceCode"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span class="co">-- source: https://github.com/mstksg/inCode/tree/master/code-samples/dependent-haskell/NetworkTyped.hs#L57-70</span>
<span class="ot">randomWeights ::</span> (<span class="dt">MonadRandom</span> m, <span class="dt">KnownNat</span> i, <span class="dt">KnownNat</span> o)
              <span class="ot">=&gt;</span> m (<span class="dt">Weights</span> i o)
randomWeights <span class="fu">=</span> <span class="kw">do</span>
    s1 <span class="ot">&lt;-</span> getRandom
    s2 <span class="ot">&lt;-</span> getRandom
    <span class="kw">let</span> wB <span class="fu">=</span> randomVector s1 <span class="dt">Uniform</span> <span class="fu">*</span> <span class="dv">2</span> <span class="fu">-</span> <span class="dv">1</span>
        wN <span class="fu">=</span> uniformSample s2 (<span class="fu">-</span><span class="dv">1</span>) <span class="dv">1</span>
    return <span class="fu">$</span> <span class="dt">W</span> wB wN

<span class="ot">randomNet ::</span> forall m i hs o<span class="fu">.</span> (<span class="dt">MonadRandom</span> m, <span class="dt">KnownNat</span> i, <span class="dt">KnownNats</span> hs, <span class="dt">KnownNat</span> o)
          <span class="ot">=&gt;</span> m (<span class="dt">Network</span> i hs o)
randomNet <span class="fu">=</span> <span class="kw">case</span><span class="ot"> natsList ::</span> <span class="dt">NatList</span> hs <span class="kw">of</span>
              Ø<span class="dt">NL</span>     <span class="ot">-&gt;</span> <span class="dt">O</span>     <span class="fu">&lt;$&gt;</span> randomWeights
              _ <span class="fu">:&lt;#</span> _ <span class="ot">-&gt;</span> (<span class="fu">:&amp;~</span>) <span class="fu">&lt;$&gt;</span> randomWeights <span class="fu">&lt;*&gt;</span> randomNet</code></pre></div>
<p>Notice that the <code>Static</code> versions of <code>randomVector</code> and <code>uniformSample</code> don’t actually require the size of the vector/matrix you want as an input – they just use type inference to figure out what size you want! This is the same process that <code>read</code> uses to figure out what type of thing you want to return. You would use <code>randomVector s Uniform :: R 10</code>, and type inference would give you a 10-element vector the same way <code>read &quot;hello&quot; :: Int</code> would give you an <code>Int</code>.</p></div><footer><ul class="entry-series"></ul><ul class="tag-list"><li><a href="https://blog.jle.im/entries/tagged/functional-programming.html" class="tag-a-tag">#functional programming</a></li><li><a href="https://blog.jle.im/entries/tagged/haskell.html" class="tag-a-tag">#haskell</a></li><li><a href="https://blog.jle.im/entries/tagged/types.html" class="tag-a-tag">#types</a></li><li><a href="https://blog.jle.im/entries/category/@haskell.html" class="tag-a-category">@HASKELL</a></li><li><a href="https://blog.jle.im/entries/category/@ramblings.html" class="tag-a-category">@RAMBLINGS</a></li></ul><aside class="social-buttons"><div class="addthis_toolbox addthis_default_style addthis-buttons"><a class="addthis_button_facebook_like" fb:like:layout="button_count"></a><a class="addthis_button_tweet"></a><a class="addthis_button_google_plusone" g:plusone:size="medium"></a><a class="addthis_counter addthis_pill_style"></a></div><div class="custom-social-buttons"><div class="custom-social-button"><a href="https://www.reddit.com/submit" onclick="window.location = &#39;https://www.reddit.com/submit?url=&#39;+ encodeURIComponent(window.location); return false"><img src="https://www.reddit.com/static/spreddit7.gif" alt="submit to reddit"></a></div></div></aside><nav class="next-prev-links"><ul><li class="next-entry-link">(Next) <a href="https://blog.jle.im/entry/introducing-in-code.html">Introducing “in Code”!</a> &rarr;</li></ul></nav></footer></article><div class="post-entry"><div class="tile"><div id="disqus_thread"></div><script type="text/javascript">var disqus_config = function () {
    this.page.url = 'https://blog.jle.im/entry/practical-dependent-types-in-haskell-1.html';
    this.page.identifier = 'dependent-haskell-1';
};
(function() {
    var d = document, s = d.createElement('script');
    s.src = '//incode.disqus.com/embed.js';
    s.setAttribute('data-timestamp', +new Date());
    (d.head || d.body).appendChild(s);
})();
</script><noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a><br></noscript><a href="http://disqus.com" class="dsq-brlink">Comments powered by <span class="logo-disqus">Disqus</span></a></div></div></div></div></div><div id="footer-container"><div id="footer-content"><div class="tile"><div class="footer-copyright">&copy; 2016 Justin Le</div><div class="footer-follow social-follows"><ul class="social-follows-list"><li><ul class="social-follows-list-social"><li><a class="social-follow-facebook" title="Friend me on Facebook!" href="https://facebook.com/mstksg">Facebook</a></li><li><a class="social-follow-twitter" title="Follow me on Twitter!" href="https://twitter.com/intent/user?user_id=mstk" onclick="window.open(
  &#39;http://twitter.com/intent/user?user_id=907281&#39;,
  &#39;facebook-share-dialog&#39;,
  &#39;width=550,height=520&#39;);
return false;
">Twitter</a></li><li><a class="social-follow-gplus" title="Add me on Google+!" href="https://plus.google.com/+JustinLe">Google+</a></li><li><a class="social-follow-linkedin" title="Connect with me on LinkedIn!" href="https://linkedin.com/in/lejustin">LinkedIn</a></li><li><a class="social-follow-github" title="Fork me on Github!" href="https://github.com/mstksg">Github</a></li><li><a class="social-follow-keybase" title="Track me on Keybase!" href="https://keybase.io/mstksg">Keybase</a></li><li><a class="social-follow-bitcoin" title="Donate via bitcoin!" href="https://coinbase.com/mstksg">Bitcoin</a></li></ul></li><li><ul class="social-follows-list-site"><li><a class="social-follow-rss" title="Subscribe to my RSS Feed!" href="http://feeds.feedburner.com/incodeblog">RSS</a></li><li><a class="social-follow-email" title="Subscribe to the mailing list!" href="https://feedburner.google.com/fb/a/mailverify?loc=en_US&amp;uri=incodeblog">Mailing list</a></li></ul></li></ul></div></div></div></div></body></html>