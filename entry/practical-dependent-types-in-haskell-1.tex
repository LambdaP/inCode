\documentclass[]{article}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
\else % if luatex or xelatex
  \ifxetex
    \usepackage{mathspec}
    \usepackage{xltxtra,xunicode}
  \else
    \usepackage{fontspec}
  \fi
  \defaultfontfeatures{Mapping=tex-text,Scale=MatchLowercase}
  \newcommand{\euro}{â‚¬}
\fi
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{\usepackage{microtype}}{}
\usepackage[margin=1in]{geometry}
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\newenvironment{Shaded}{}{}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{{#1}}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{{#1}}}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{{#1}}}
\newcommand{\ImportTok}[1]{{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{{#1}}}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{{#1}}}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{{#1}}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{{#1}}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{{#1}}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{{#1}}}
\newcommand{\BuiltInTok}[1]{{#1}}
\newcommand{\ExtensionTok}[1]{{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{{#1}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{{#1}}}
\newcommand{\RegionMarkerTok}[1]{{#1}}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
\newcommand{\NormalTok}[1]{{#1}}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
\ifxetex
  \usepackage[setpagesize=false, % page size defined by xetex
              unicode=false, % unicode breaks when used with xetex
              xetex]{hyperref}
\else
  \usepackage[unicode=true]{hyperref}
\fi
\hypersetup{breaklinks=true,
            bookmarks=true,
            pdfauthor={Justin Le},
            pdftitle={Practical Dependent Types in Haskell: Type-Safe Neural Networks},
            colorlinks=true,
            citecolor=blue,
            urlcolor=blue,
            linkcolor=magenta,
            pdfborder={0 0 0}}
\urlstyle{same}  % don't use monospace font for urls
% Make links footnotes instead of hotlinks:
\renewcommand{\href}[2]{#2\footnote{\url{#1}}}
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\setcounter{secnumdepth}{0}

\title{Practical Dependent Types in Haskell: Type-Safe Neural Networks}
\author{Justin Le}

\begin{document}
\maketitle

\emph{Originally posted on \textbf{\href{https://blog.jle.im/}{in
Code}}.}

Whether you like it or not, programming with dependent types in Haskell
moving slowly but steadily to the mainstream of Haskell programming. In
the current state of Haskell education, dependent types are often
considered topics for ``advanced'' Haskell users. However, I can
definitely foresee a day where the ease of use of modern Haskell
libraries relying on dependent types as well as their ubiquitousness
forces programming with dependent types to be an integral part of
regular intermediate (or even beginner) Haskell education, as much as
Traversable or Maps.

he point of this post is to show some practical examples of using
dependent types in the real world, and to also walk through the ``why''
and high-level philosophy of the way you structure your Haskell
programs. It'll also hopefully instill an intuition of a dependently
typed work flow of ``exploring'' how dependent types can help your
current programs.

The first project in this series will build up to type-safe
\textbf{\href{https://en.wikipedia.org/wiki/Artificial_neural_network}{artificial
neural network}} implementations. Hooray!

There are other great tutorials I'd recommend online if you want to
explore dependent types in Haskell further, including
\href{http://www.well-typed.com/blog/2015/11/implementing-a-minimal-version-of-haskell-servant/}{this
great servant ``tutorial''}. Also, I should provide a disclaimer --- I'm
also currently exploring all of this as I'm going along too. It's a wild
world out there. Join me and let's be a part of the frontier!

\section{Neural Networks}\label{neural-networks}

\href{https://en.wikipedia.org/wiki/Artificial_neural_network}{Artificial
neural networks} have been somewhat of a hot topic in computing
recently. At their core they involve matrix multiplication and
manipulation, so they do seem like a good candidate for a dependent
types. Most importantly, implementations of training algorithms (like
back-propagation) are tricky to implement correctly --- despite being
simple, there are many locations where accidental bugs might pop up when
multiplying the wrong matrices, for example.

However, it's not always easy to gauge before-the-fact what would or
would not be a good candidate for adding dependent types to, and often
times, it can be considered premature to start off with ``as powerful
types as you can''. So we'll walk through a simple implementation
\emph{without}, and see all of the red flags that hint that you might
want to start considering stronger types.

Edwin Brady calls this process ``type-driven development''. Start
general, recognize the partial functions and red flags, and slowly add
more powerful types.

\section{Vanilla Types}\label{vanilla-types}

\begin{figure}[htbp]
\centering
\includegraphics{/img/entries/dependent-haskell-1/ffneural.png}
\caption{Feed-forward ANN architecture}
\end{figure}

We're going to be implementing a feed-forward neural network, with
back-propagation training. These networks are layers of ``nodes'', each
connected to the each of the nodes of the previous layer.

Input goes to the first layer, which feeds information to the next year,
which feeds it to the next, etc., until the final layer, where we read
it off as the ``answer'' that the network is giving us.

Every node ``outputs'' a weighted sum of all of the outputs of the
\emph{previous} layer, plus an always-on ``bias'' term (so that its
result can be non-zero even when all of its inputs are zero).
Mathematically, it looks like:

\[
y_j = b_j + \sum_i^m w_{ij} x_i
\]

Or, if we treat the output of a layer and the list of list of weights as
a matrix, we can write it a little cleaner:

\[
\mathbf{y} = \mathbf{b} + \hat{W} \mathbf{x}
\] To ``scale'' the result (and to give the system the magical powers of
nonlinearity), we actually apply an ``activation function'' to the
output before passing it down to the next step. We'll be using the
popular \href{https://en.wikipedia.org/wiki/Logistic_function}{logistic
function}, \(f(x) = 1 / (1 + e^{-x})\).

\emph{Training} a network involves picking the right set of weights to
get the network to answer the question you want.

We can store a network by storing the matrix of of weights and biases
between each layer:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{-- source: https://github.com/mstksg/inCode/tree/master/code-samples/dependent-haskell/NetworkUntyped.hs#L13-16}
\KeywordTok{data} \DataTypeTok{Weights} \FunctionTok{=} \DataTypeTok{W} \NormalTok{\{}\OtherTok{ wBiases ::} \FunctionTok{!}\NormalTok{(}\DataTypeTok{Vector} \DataTypeTok{Double}\NormalTok{)}
                 \NormalTok{,}\OtherTok{ wNodes  ::} \FunctionTok{!}\NormalTok{(}\DataTypeTok{Matrix} \DataTypeTok{Double}\NormalTok{)}
                 \NormalTok{\}}
  \KeywordTok{deriving} \NormalTok{(}\DataTypeTok{Show}\NormalTok{, }\DataTypeTok{Eq}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Now, a \texttt{Weights} linking an \emph{n}-node layer to an
\emph{m}-node layer has an \emph{m}-dimensional bias vector (one
component for each output) and an \emph{m}-by-\emph{n} node weight
matrix (one column for each output, one row for each input).

(We're using the \texttt{Matrix} type from the awesome
\emph{\href{http://hackage.haskell.org/package/hmatrix}{hmatrix}}
library for linear algebra, implemented using blas/lapack under the
hood)

\end{document}
