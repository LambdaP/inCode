\documentclass[]{article}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
\else % if luatex or xelatex
  \ifxetex
    \usepackage{mathspec}
    \usepackage{xltxtra,xunicode}
  \else
    \usepackage{fontspec}
  \fi
  \defaultfontfeatures{Mapping=tex-text,Scale=MatchLowercase}
  \newcommand{\euro}{â‚¬}
\fi
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{\usepackage{microtype}}{}
\usepackage[margin=1in]{geometry}
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\newenvironment{Shaded}{}{}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{{#1}}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{{#1}}}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{{#1}}}
\newcommand{\ImportTok}[1]{{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{{#1}}}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{{#1}}}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{{#1}}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{{#1}}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{{#1}}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{{#1}}}
\newcommand{\BuiltInTok}[1]{{#1}}
\newcommand{\ExtensionTok}[1]{{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{{#1}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{{#1}}}
\newcommand{\RegionMarkerTok}[1]{{#1}}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
\newcommand{\NormalTok}[1]{{#1}}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
\ifxetex
  \usepackage[setpagesize=false, % page size defined by xetex
              unicode=false, % unicode breaks when used with xetex
              xetex]{hyperref}
\else
  \usepackage[unicode=true]{hyperref}
\fi
\hypersetup{breaklinks=true,
            bookmarks=true,
            pdfauthor={Justin Le},
            pdftitle={Practical Dependent Types in Haskell: Type-Safe Neural Networks},
            colorlinks=true,
            citecolor=blue,
            urlcolor=blue,
            linkcolor=magenta,
            pdfborder={0 0 0}}
\urlstyle{same}  % don't use monospace font for urls
% Make links footnotes instead of hotlinks:
\renewcommand{\href}[2]{#2\footnote{\url{#1}}}
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\setcounter{secnumdepth}{0}

\title{Practical Dependent Types in Haskell: Type-Safe Neural Networks}
\author{Justin Le}

\begin{document}
\maketitle

\emph{Originally posted on \textbf{\href{https://blog.jle.im/}{in
Code}}.}

Whether you like it or not, programming with dependent types in Haskell
moving slowly but steadily to the mainstream of Haskell programming. In
the current state of Haskell education, dependent types are often
considered topics for ``advanced'' Haskell users. However, I can
definitely foresee a day where the ease of use of modern Haskell
libraries relying on dependent types as well as their ubiquitousness
forces programming with dependent types to be an integral part of
regular intermediate (or even beginner) Haskell education, as much as
Traversable or Maps.

However, I feel like most ``dependent typing'' tutorials I see around
the internet focus on things like proofs and theorems, instead of
directly jumping into how they can be used to help you in your current
coding now.

So, the point of this post is to show some practical examples of using
dependent types in the real world, and to also walk through the ``why''
and high-level philosophy of the way you structure your Haskell
programs. It'll also hopefully instill an intuition of a dependently
typed work flow of ``exploring'' how dependent types can help your
current programs.

The first project in this series will build up to type-safe
\textbf{\href{https://en.wikipedia.org/wiki/Artificial_neural_network}{artificial
neural network}} implementations. Hooray!

There are other great tutorials I'd recommend online if you want to
explore dependent types in Haskell further, including
\href{http://www.well-typed.com/blog/2015/11/implementing-a-minimal-version-of-haskell-servant/}{this
great servant ``tutorial''}. Also, I should provide a disclaimer --- I'm
also currently exploring all of this as I'm going along too. It's a wild
world out there. Join me and let's be a part of the frontier!

\section{Neural Networks}\label{neural-networks}

\href{https://en.wikipedia.org/wiki/Artificial_neural_network}{Artificial
neural networks} have been somewhat of a hot topic in computing
recently. At their core they involve matrix multiplication and
manipulation, so they do seem like a good candidate for a dependent
types. Most importantly, implementations of training algorithms (like
back-propagation) are tricky to implement correctly --- despite being
simple, there are many locations where accidental bugs might pop up when
multiplying the wrong matrices, for example.

However, it's not always easy to gauge before-the-fact what would or
would not be a good candidate for adding dependent types to, and often
times, it can be considered premature to start off with ``as powerful
types as you can''. So we'll walk through a simple implementation
\emph{without}, and see all of the red flags that hint that you might
want to start considering stronger types.

\section{Vanilla Types}\label{vanilla-types}

\begin{figure}[htbp]
\centering
\includegraphics{/img/entries/dependent-haskell-1/ffneural.png}
\caption{Feed-forward ANN architecture}
\end{figure}

We're going to be implementing a feed-forward neural network, with
back-propagation training. A feed-forward neural network consists
structurally of layers of ``nodes'', each connected to the each of the
nodes of the previous layer and each of the nodes of the next layer. The
most important feature of the network itself is the ``strength'' of
these connections, called ``weights''. To make a prediction, each node
takes, as an input, the weighted sum of all of the outputs of the
previous layer, weighted by the connection weights (plus an additional
``bias'' shift). It then outputs a function of this weighted sum,
\(f(x)\), to be used by all of the nodes of the next layer. At the
high-level, the user feeds in an input vector to the top-level nodes,
the network processes these layer-by-layer, and the result of the final
nodes is what is taken as the network's output. The ``goal'' of
designing/training a network is to somehow pick the right set of weights
that will give the output that you want for the given input.

A picture is worth a thousand words, so the following equation
demonstrates things nicely:

\[
o_j = b_j + \sum_i x_i * w_ij
\]

While it's nice to think about neural networks in terms of their nodes,
it makes more sense computationally to only identify a network by simply
the matrices of weights alone --- let's imagine one ``layer'', which is
actually a matrix of weights from one layer to another:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{-- source: https://github.com/mstksg/inCode/tree/master/code-samples/dependent-haskell/NetworkUntyped.hs#L10-13}
\KeywordTok{data} \DataTypeTok{Weights} \FunctionTok{=} \DataTypeTok{W} \NormalTok{\{}\OtherTok{ wBiases  ::} \FunctionTok{!}\NormalTok{(}\DataTypeTok{Vector} \DataTypeTok{Double}\NormalTok{)}
                 \NormalTok{,}\OtherTok{ wWeights ::} \FunctionTok{!}\NormalTok{(}\DataTypeTok{Matrix} \DataTypeTok{Double}\NormalTok{)}
                 \NormalTok{\}}
  \KeywordTok{deriving} \NormalTok{(}\DataTypeTok{Show}\NormalTok{, }\DataTypeTok{Eq}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Now, a \texttt{Weights} linking a layer of \(n\) nodes to a layer of
\(m\) nodes will have a bias vector of size \(m\) (the bias shift for
each of the output nodes) and a weight matrix of size \(m \times n\).

(We're using the \texttt{Matrix} type from the awesome
\emph{\href{http://hackage.haskell.org/package/hmatrix}{hmatrix}}
library for linear algebra, implemented using blas/lapack under the
hood)

Now let's represent a feed-forward network:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{-- source: https://github.com/mstksg/inCode/tree/master/code-samples/dependent-haskell/NetworkUntyped.hs#L15-18}
\KeywordTok{data} \DataTypeTok{Network} \FunctionTok{=} \DataTypeTok{O} \FunctionTok{!}\DataTypeTok{Weights}
             \FunctionTok{|} \FunctionTok{!}\DataTypeTok{Weights} \FunctionTok{:&~} \FunctionTok{!}\DataTypeTok{Network}
  \KeywordTok{deriving} \NormalTok{(}\DataTypeTok{Show}\NormalTok{, }\DataTypeTok{Eq}\NormalTok{)}
\KeywordTok{infixr} \DecValTok{5} \FunctionTok{:&~}
\end{Highlighting}
\end{Shaded}

So a network with one input layer, two inner layers, and one output
layer would look like:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{i2h }\FunctionTok{:&~} \NormalTok{h2h }\FunctionTok{:&~} \DataTypeTok{O} \NormalTok{h2o}
\end{Highlighting}
\end{Shaded}

Where the first component is the weights from the input to the first
hidden layer, the second is the weights from the first hidden layer to
the second, and the final is the weights from the second hidden layer to
the outputs.

TODO: graphs using diagrams?

We can write simple procedures, like generating random networks:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{-- source: https://github.com/mstksg/inCode/tree/master/code-samples/dependent-haskell/NetworkUntyped.hs#L36-46}
\OtherTok{randomWeights ::} \DataTypeTok{MonadRandom} \NormalTok{m }\OtherTok{=>} \DataTypeTok{Int} \OtherTok{->} \DataTypeTok{Int} \OtherTok{->} \NormalTok{m }\DataTypeTok{Weights}
\NormalTok{randomWeights i o }\FunctionTok{=} \KeywordTok{do}
    \NormalTok{s1 }\OtherTok{<-} \NormalTok{getRandom}
    \NormalTok{s2 }\OtherTok{<-} \NormalTok{getRandom}
    \KeywordTok{let} \NormalTok{wBiases  }\FunctionTok{=} \NormalTok{randomVector s1 }\DataTypeTok{Uniform} \NormalTok{o }\FunctionTok{*} \DecValTok{2} \FunctionTok{-} \DecValTok{1}
        \NormalTok{wWeights }\FunctionTok{=} \NormalTok{uniformSample s2 o (replicate i (}\FunctionTok{-}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{))}
    \NormalTok{return }\DataTypeTok{W}\NormalTok{\{}\FunctionTok{..}\NormalTok{\}}

\OtherTok{randomNet ::} \DataTypeTok{MonadRandom} \NormalTok{m }\OtherTok{=>} \DataTypeTok{Int} \OtherTok{->} \NormalTok{[}\DataTypeTok{Int}\NormalTok{] }\OtherTok{->} \DataTypeTok{Int} \OtherTok{->} \NormalTok{m }\DataTypeTok{Network}
\NormalTok{randomNet i [] o     }\FunctionTok{=}     \DataTypeTok{O} \FunctionTok{<$>} \NormalTok{randomWeights i o}
\NormalTok{randomNet i (h}\FunctionTok{:}\NormalTok{hs) o }\FunctionTok{=} \NormalTok{(}\FunctionTok{:&~}\NormalTok{) }\FunctionTok{<$>} \NormalTok{randomWeights i h }\FunctionTok{<*>} \NormalTok{randomNet h hs o}
\end{Highlighting}
\end{Shaded}

(\texttt{randomVector} and \texttt{uniformSample} are from the
\emph{hmatrix} library, generating random vectors and matrices from a
random \texttt{Int} seed. We configure them to generate them with
numbers between -1 and 1)

And now a function to ``run'' our network on a given input vector:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{-- source: https://github.com/mstksg/inCode/tree/master/code-samples/dependent-haskell/NetworkUntyped.hs#L20-34}
\OtherTok{logistic ::} \DataTypeTok{Double} \OtherTok{->} \DataTypeTok{Double}
\NormalTok{logistic x }\FunctionTok{=} \DecValTok{1} \FunctionTok{/} \NormalTok{(}\DecValTok{1} \FunctionTok{+} \NormalTok{exp (}\FunctionTok{-}\NormalTok{x))}

\OtherTok{runLayer ::} \DataTypeTok{Weights} \OtherTok{->} \DataTypeTok{Vector} \DataTypeTok{Double} \OtherTok{->} \DataTypeTok{Vector} \DataTypeTok{Double}
\NormalTok{runLayer (}\DataTypeTok{W} \NormalTok{wB wW) v }\FunctionTok{=} \NormalTok{wB }\FunctionTok{+} \NormalTok{(wW }\FunctionTok{#>} \NormalTok{v)}

\OtherTok{runNet ::} \DataTypeTok{Network} \OtherTok{->} \DataTypeTok{Vector} \DataTypeTok{Double} \OtherTok{->} \DataTypeTok{Vector} \DataTypeTok{Double}
\NormalTok{runNet (}\DataTypeTok{O} \NormalTok{w)      }\FunctionTok{!}\NormalTok{v }\FunctionTok{=} \NormalTok{logistic }\OtherTok{`cmap`} \NormalTok{runLayer w v}
\NormalTok{runNet (w }\FunctionTok{:&~} \NormalTok{n') }\FunctionTok{!}\NormalTok{v }\FunctionTok{=} \KeywordTok{let} \NormalTok{v' }\FunctionTok{=} \NormalTok{logistic }\OtherTok{`cmap`} \NormalTok{runLayer w v}
                       \KeywordTok{in}  \NormalTok{runNet n' v'}
\end{Highlighting}
\end{Shaded}

(\texttt{\#\textgreater{}} is matrix-vector multiplication)

TODO: examples of running

If you're a normal programmer, this might seem perfectly fine. If you
are a Haskell programmer, you should already be having heart attacks.
Let's imagine all of the bad things that could happen:

\begin{itemize}
\item
  How do we even know that each subsequent matrix in the network is
  ``compatible''? We want the outputs of one matrix to line up with the
  inputs of the next, but there's no way to know unless we have ``smart
  constructors'' to check while we add things. But it's possible to
  build a bad network, and things will just explode at runtime.
\item
  How do we know the size vector the network expects? What stops you
  from sending in a bad vector at run-time and having everything
  explode?
\item
  How do we verify that we have implemented \texttt{runLayer} and
  \texttt{runNet} in a way that they won't suddenly fail at runtime? We
  write \texttt{l\ \#\textgreater{}\ v}, but how do we know that it's
  even correct? We can it prove ourselves, but the compiler won't help
  us.
\end{itemize}

Now, let's try implementing back-propagation:

\begin{Shaded}
\begin{Highlighting}[]
\OtherTok{train ::} \DataTypeTok{Vector} \DataTypeTok{Double} \OtherTok{->} \DataTypeTok{Vector} \DataTypeTok{Double} \OtherTok{->} \DataTypeTok{Network} \OtherTok{->} \DataTypeTok{Network}
\NormalTok{train i o }\FunctionTok{=} \NormalTok{go i}
  \KeywordTok{where}
\OtherTok{    go ::} \DataTypeTok{Vector} \DataTypeTok{Double} \OtherTok{->} \DataTypeTok{Network} \OtherTok{->} \NormalTok{(}\DataTypeTok{Vector} \DataTypeTok{Double}\NormalTok{, }\DataTypeTok{Network}\NormalTok{)}
    \NormalTok{go }\FunctionTok{=} \NormalTok{undefined}
\end{Highlighting}
\end{Shaded}

\end{document}
